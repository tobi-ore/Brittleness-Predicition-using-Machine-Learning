{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning to Predict Brittleness from other Geophysical Logs\n",
    "\n",
    "## Data: 4 wells from the Appalachian Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbR, GradientBoostingClassifier as gbC, IsolationForest\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "pd.set_option('display.max_columns', None)   #to display all the column information\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = r\"../Thesis work/Thesis work/Well_Data_CSV_Merged\"    #for macbook google drive\n",
    "\n",
    "file_name1 = \"Poseidon.csv\"\n",
    "file_name2 = \"Boggess.csv\"\n",
    "file_name3 = \"Mip3h.csv\"\n",
    "file_name4 = \"Whipkey.csv\"\n",
    "\n",
    "file_name = [file_name1, file_name2, file_name3, file_name4]\n",
    "data = []\n",
    "\n",
    "for i in file_name:\n",
    "    file_path = os.path.join(file_directory,i)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_poseidon = data[0]\n",
    "data_boggess = data[1]\n",
    "data_mip3h = data[2]\n",
    "data_whipkey = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Marcellus Shale interval\n",
    "# data_poseidon = data_poseidon.loc[(data_poseidon['DEPT'] > 7880) & (data_poseidon['DEPT'] < 8040)]\n",
    "# data_boggess = data_boggess.loc[(data_boggess['DEPT'] > 7880) & (data_boggess['DEPT'] < 7970)]\n",
    "# data_mip3h = data_mip3h.loc[(data_mip3h['DEPT'] > 7450) & (data_mip3h['DEPT'] < 7560)]\n",
    "# data_whipkey = data_whipkey.loc[(data_whipkey['DEPT'] > 7730) & (data_whipkey['DEPT'] < 7840)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Poseidon data has {} rows\".format(data_poseidon.shape[0]))\n",
    "print(\"The Boggess data has {} rows\".format(data_boggess.shape[0]))\n",
    "print(\"The Mip3h data has {} rows\".format(data_mip3h.shape[0]))\n",
    "print(\"The Whipkey data has {} rows\".format(data_whipkey.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['DEPT', 'GR', 'NPHI','RHOZ', 'HCAL', 'DTCO','PEFZ','Brittleness_new']   #list of the features names to select\n",
    "# features =  ['DEPT', 'GR','RHOZ', 'HCAL', 'NPHI','DTCO', 'Brittleness_new']   #list of the features names to select\n",
    "target = 'Brittleness_new'   #name of the output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_whipkey,\n",
    "                  data_boggess,\n",
    "                  data_poseidon], ignore_index=True)\n",
    "data = data.loc[: ,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (15,6))\n",
    "\n",
    "m = ax[0].scatter(data_poseidon.PR_DYN, data_poseidon.YME_DYN, c = data_poseidon.Brittleness)\n",
    "ax[0].set_xlabel(\"Poisson's ratio\", fontsize =15)\n",
    "ax[0].set_ylabel(\"Young's modulus\", fontsize =15)\n",
    "ax[0].axhline(y=6, color='r', linestyle='--')\n",
    "ax[0].axvline(x=0.2, color='r', linestyle='--')\n",
    "ax[0].text(0.23, 0.6, 'Brittle Region',fontsize=15, horizontalalignment='center', verticalalignment='center', transform=ax[0].transAxes, c='r')\n",
    "ax[0].text(0.75, 0.08, 'Ductile Region',fontsize=15, horizontalalignment='center', verticalalignment='center', transform=ax[0].transAxes, c='r')\n",
    "\n",
    "l = ax[1].scatter(data_poseidon.PR_DYN, data_poseidon.YME_DYN, c = data_poseidon.Brittleness_new)\n",
    "ax[1].set_xlabel(\"Poisson's ratio\", fontsize =15)\n",
    "ax[1].set_ylabel(\"Young's modulus\", fontsize =15)\n",
    "ax[1].axhline(y=6, color='r', linestyle='--')\n",
    "ax[1].axvline(x=0.2, color='r', linestyle='--')\n",
    "ax[1].text(0.23, 0.6, 'Brittle Region',fontsize=15, horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes, c='r')\n",
    "ax[1].text(0.75, 0.08, 'Ductile Region',fontsize=15, horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes, c='r')\n",
    "\n",
    "fig.colorbar(l, ax = ax[1])\n",
    "fig.colorbar(m, ax = ax[0])\n",
    "\n",
    "# fig.savefig(r'./Images/{}.png'.format('YME-PR plot'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data < 0] = np.nan  #remove negative values\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add correlation plot\n",
    "data.corr(method = 'spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StatRelat(data, target):\n",
    "    #Mutual information and Pearson's corelation for measuring the dependency between the variables.\n",
    "    \"\"\"\n",
    "    function to estimate the Mutual information and Pearson's corelation \n",
    "    for measuring the dependency between the variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The data \n",
    "    target: Str\n",
    "        The column name of the target feature\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A histogram of mutual information and heatmap of correlation between features\n",
    "    \"\"\"\n",
    "    df2 = data.copy().dropna()\n",
    "    X = df2.drop(['DEPT',target], axis=1)._get_numeric_data()   # separate DataFrames for predictor and response features\n",
    "    y = df2.loc[:,[target]]._get_numeric_data()\n",
    "\n",
    "    mi = mutual_info_regression(X,np.ravel(y), random_state=20) # calculate mutual information\n",
    "    mi /= np.max(mi)                          # calculate relative mutual information\n",
    "\n",
    "    indices = np.argsort(mi)[::-1]            # find indicies for descending order\n",
    "\n",
    "    print(\"Feature ranking:\")                 # write out the feature importances\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %s = %f\" % (f + 1, X.columns[indices][f], mi[indices[f]]))\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(15, 7))\n",
    "#     fig.subplots_adjust(left=0.0, bottom=0.0, right=1., top=1., wspace=0.2, hspace=0.2)\n",
    "    \n",
    "    ax[0].bar(range(X.shape[1]), mi[indices],color=\"g\", align=\"center\")\n",
    "    ax[0].set_title(\"Mutual Information\")\n",
    "    ax[0].set_xticks(range(X.shape[1]))\n",
    "    ax[0].set_xticklabels(X.columns[indices],rotation=90)\n",
    "    ax[0].set_xlim([-1, X.shape[1]])\n",
    "    \n",
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "    mask = np.zeros_like(df2.drop(['DEPT'], axis=1).corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "        sns.heatmap(df2.drop(['DEPT'], axis=1).corr(), mask=mask,cmap=cmap, vmax=.3, ax=ax[1], square=True, annot = True)\n",
    "        ax[1].set_yticklabels(ax[1].get_yticklabels(), rotation=45)\n",
    "    \n",
    "    \n",
    "    fig.savefig(r'./Images/{}.png'.format('feature_selection'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatRelat(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = data.drop(['DEPT'], axis=1).describe().T.round(2)\n",
    "# data_summary.to_excel(r'./Images/{}.xlsx'.format('data_summary_before_stand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#range\n",
    "data_summary['max'] - data_summary['min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard deviation\n",
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_norm = pd.DataFrame(scaler.fit_transform(data.drop(['DEPT'], axis=1)), columns = data.drop(['DEPT'], axis=1).columns)\n",
    "data_norm_summary = data_norm.describe().T.round(2)\n",
    "data_norm_summary\n",
    "# data_norm_summary.to_excel(r'./Images/{}.xlsx'.format('data_summary_after_minmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_norm = pd.DataFrame(scaler.fit_transform(data.drop(['DEPT'], axis=1)), columns = data.drop(['DEPT'], axis=1).columns)\n",
    "data_norm_summary = data_norm.describe().T.round(2)\n",
    "data_norm_summary\n",
    "# data_norm_summary.to_excel(r'./Images/{}.xlsx'.format('data_summary_after_standard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['DEPT','RHOZ',target], axis=1)\n",
    "y = data.loc[:,[target]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot(X_train, save_file_name):\n",
    "    fig, ax = plt.subplots(1,len(X_train.columns), figsize = (15,8))\n",
    "\n",
    "\n",
    "    for i, feature in enumerate(X_train.columns):\n",
    "        ax[i].boxplot(X_train[feature])\n",
    "        ax[i].set_ylabel(feature, fontsize = 20)\n",
    "        right_side = ax[i].spines[\"right\"]\n",
    "        top_side = ax[i].spines[\"top\"]\n",
    "        bottom_side = ax[i].spines[\"bottom\"]\n",
    "\n",
    "\n",
    "\n",
    "        right_side.set_visible(False)\n",
    "        top_side.set_visible(False)\n",
    "        bottom_side.set_visible(False)\n",
    "        ax[i].axes.get_xaxis().set_visible(False)\n",
    "#     fig.savefig(r'./Images/{}.png'.format(save_file_name), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(X_train, \"before_outlier_removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(X_train, \"after_outlier_removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_mip3h.loc[: ,features].dropna()\n",
    "X_blind = df.drop(['DEPT','RHOZ',target], axis=1)\n",
    "y_blind = df.loc[:,[target]]\n",
    "\n",
    "# X_blind = data_boggess.loc[: ,features].drop([target], axis=1)\n",
    "# y_blind = data_boggess.loc[:,[target]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(X_train, X_test, X_blind, y_train, y_test, y_blind, algorithm, hyper_parameters, scaler, classification,printFeatureImportance=True, cv_folds=3):\n",
    "    \"\"\"\n",
    "    function to tune the gradient boosting model and return the optimum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : DataFrame\n",
    "        The input features for the training set\n",
    "    X_test : DataFrame\n",
    "        The input features for the testing set\n",
    "    X_blind : DataFrame\n",
    "        The input features for the blind set\n",
    "    y_train : DataFrame \n",
    "        The output feature for the training set\n",
    "    y_test : DataFrame \n",
    "        The output feature for the testing set\n",
    "    y_blind : DataFrame \n",
    "        The output feature for the blind set\n",
    "    algorithm : {'neural','svm','gradientboosting'}\n",
    "        The Machine Learning model \n",
    "    hyper_parameters : dict\n",
    "        A dictionary of the hyperparameters of the models that will be tuned\n",
    "    scaler : {'standard','minmax'}\n",
    "        Scaling technique to employ.\n",
    "    classification : bool \n",
    "        Flag to specify the modeling technique. True for classification and False for regression\n",
    "    printFeatureImportance : bool\n",
    "        Flag to specify if to display the feature importance histogram.\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds. default is 3.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : an object of the trained gradient boosting which can be deployed or saved\n",
    "    \"\"\"   \n",
    "    #step to assign the selected standardaziation \n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler == 'minmax':\n",
    "        scaler =MinMaxScaler()\n",
    "    else:\n",
    "        print(\"invalid scaler: use 'standard' or 'minmax'\")\n",
    "        \n",
    "    \n",
    "    #step to assign the selected machine learning algorithm\n",
    "    if algorithm == 'svm':\n",
    "        if classification is True:\n",
    "            algo = SVC(random_state=83)\n",
    "        else:\n",
    "            algo = SVR()\n",
    "        \n",
    "    elif algorithm == 'neural':\n",
    "        if classification is True:\n",
    "            algo = MLPClassifier(random_state=677)\n",
    "        else:\n",
    "            algo = MLPRegressor(random_state=134)\n",
    "    \n",
    "    elif algorithm == 'gradientboosting':\n",
    "        if classification is True:\n",
    "            algo = gbC(random_state=10)\n",
    "        else:\n",
    "            algo = gbR(random_state=824)\n",
    "    else:\n",
    "        print(\"invalid scaler: use 'svm' or 'neural' or 'gradientboosting'\")\n",
    "    \n",
    "     \n",
    "        \n",
    "    if classification is True:\n",
    "        pipe = Pipeline(steps=[('scaler', scaler), ('model', algo)])\n",
    "        model = GridSearchCV(estimator = pipe,\n",
    "                             param_grid = hyper_parameters,\n",
    "                             scoring='accuracy',n_jobs=-1, cv=cv_folds, verbose = 1)\n",
    "        \n",
    "        #Fit the model on the data\n",
    "        model.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "        #Predict training set:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "        #Predict testing set:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        #Predict blind set\n",
    "        y_blind_pred = model.predict(X_blind)\n",
    "\n",
    "        #Print model report:\n",
    "        print(\"Model Report\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"The training accuracy : {0:.4g}\".format(metrics.accuracy_score(y_train.values, y_train_pred)))\n",
    "        print(\"The testing accuracy is : {0:.4g}\".format(metrics.accuracy_score(y_test.values,y_test_pred)))\n",
    "        print(\"The blind well accuracy is : {0:.4g}\".format(metrics.accuracy_score(y_blind.values,y_blind_pred)))\n",
    "        print(\"CV best score : {0:.4g}\".format(model.best_score_))\n",
    "        print(\"CV best parameter combinations : {}\".format(model.best_params_))\n",
    "        \n",
    "        if algorithm == 'gradientboosting':\n",
    "            #Print Feature Importance:\n",
    "            if printFeatureImportance:\n",
    "                feat_imp = pd.Series(model.best_estimator_.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "                feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "                plt.xlabel('Feature Importance Score')\n",
    "           \n",
    "    else:\n",
    "        pipe = Pipeline(steps=[('scaler', scaler), ('model', algo)])\n",
    "        model = GridSearchCV(estimator = pipe,\n",
    "                             param_grid = hyper_parameters,\n",
    "                             scoring='r2',n_jobs=-1,\n",
    "                             cv=cv_folds, verbose = 1)\n",
    "        \n",
    "        #Fit the model on the data\n",
    "        model.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "        #Predict training set:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "        #Predict testing set:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "                        \n",
    "        #Predict blind set\n",
    "        y_blind_pred = model.predict(X_blind)\n",
    "\n",
    "        #Print model report:\n",
    "        print(\"Model Report\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"The training R2 score : {0:.4g}\".format(metrics.r2_score(y_train.values, y_train_pred)))\n",
    "        print(\"The testing R2 score is : {0:.4g}\".format(metrics.r2_score(y_test.values,y_test_pred)))\n",
    "        print(\"The blind well R2 score is : {0:.4g}\".format(metrics.r2_score(y_blind.values,y_blind_pred)))\n",
    "        print(\"CV best score : {0:.4g}\".format(model.best_score_))\n",
    "        print(\"CV best parameter combinations : {}\".format(model.best_params_))\n",
    "        \n",
    "        if algorithm == 'gradientboosting':\n",
    "            #Print Feature Importance:\n",
    "            if printFeatureImportance:\n",
    "                feat_imp = pd.Series(model.best_estimator_.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "                feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "                plt.xlabel('Feature Importance Score')\n",
    "\n",
    "    return model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"model__min_samples_split\" : [2,3,4,5],\n",
    "                     \"model__min_samples_leaf\": [1,2,3,4,5],\n",
    "                     \"model__max_depth\"        : range(4,8,1)\n",
    "                    \"model__n_estimators\"    : range(100,301,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of SVR() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters =  {\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = modelfit(X_train, X_test, X_blind, y_train, y_test, y_blind, algorithm='gradientboosting', \n",
    "             hyper_parameters=hyper_parameters, scaler='minmax', \n",
    "             classification=False,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size vs score \n",
    "\n",
    "m = Pipeline(steps=[('scaler', StandardScaler()), ('model', gbR(max_depth= 7, min_samples_leaf= 1, min_samples_split= 3))]) \n",
    "\n",
    "size = np.arange(500,X_train.shape[0], 500)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "blind_scores = []\n",
    "\n",
    "for i in size:\n",
    "    m.fit(X_train.iloc[:i,:].values, y_train.iloc[:i,:].values.ravel())\n",
    "    train_scores.append(metrics.r2_score(y_train.iloc[:i,:].values, m.predict(X_train.iloc[:i,:].values)))\n",
    "    test_scores.append(metrics.r2_score(y_test.values, m.predict(X_test)))\n",
    "    blind_scores.append(metrics.r2_score(y_blind.values, m.predict(X_blind)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(size, train_scores, label = 'train')\n",
    "plt.plot(size, test_scores, label = 'test')\n",
    "plt.plot(size, blind_scores, label = 'blind')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(model_gb.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "# plt.savefig(r'./Images/{}.png'.format('gb_feature_importance'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = modelfit(X_train2, X_test2, X_blind2, y_train2, y_test2, y_blind2, algorithm='gradientboosting', \n",
    "#              hyper_parameters=hyper_parameters, scaler='standard', \n",
    "#              classification=True,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'model__kernel': ['linear', 'poly','rbf','sigmoid']\n",
    "'model__gamma': ['scale', 'auto']\n",
    "'model__C': [1,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of SVR() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters = {\n",
    "                    'model__epsilon': np.arange(0.01,0.1,0.01)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_svm = modelfit(X_train, X_test, X_blind, y_train, y_test, y_blind, algorithm='svm', \n",
    "         hyper_parameters=hyper_parameters, scaler='standard', \n",
    "         classification=False,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm.named_steps.model.support_vectors_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size vs score \n",
    "\n",
    "m = Pipeline(steps=[('scaler', StandardScaler()), ('model', SVR(epsilon=0.02))])\n",
    "\n",
    "size = np.arange(500,X_train.shape[0], 500)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "blind_scores = []\n",
    "\n",
    "for i in size:\n",
    "    m.fit(X_train.iloc[:i,:].values, y_train.iloc[:i,:].values.ravel())\n",
    "    train_scores.append(metrics.r2_score(y_train.iloc[:i,:].values, m.predict(X_train.iloc[:i,:].values)))\n",
    "    test_scores.append(metrics.r2_score(y_test.values, m.predict(X_test)))\n",
    "    blind_scores.append(metrics.r2_score(y_blind.values, m.predict(X_blind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(size, train_scores, label = 'train')\n",
    "plt.plot(size, test_scores, label = 'test')\n",
    "plt.plot(size, blind_scores, label = 'blind')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of MLPClassifier() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters =  {'model__hidden_layer_sizes': [(10,10,),(19,19,),(20,),(20,20,)],\n",
    "                     'model__tol': [0.0001,0.00001,0.001],\n",
    "                    'model__solver': ['lbfgs'],\n",
    "                    'model__max_iter': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = modelfit(X_train, X_test, X_blind, y_train, y_test, y_blind, algorithm='neural', \n",
    "         hyper_parameters=hyper_parameters, scaler='minmax', \n",
    "         classification=False,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sample size vs score \n",
    "\n",
    "m = Pipeline(steps=[('scaler', StandardScaler()), ('model', MLPRegressor(hidden_layer_sizes= (19, 19), max_iter= 1000, solver= 'lbfgs', tol= 1e-05))])\n",
    "\n",
    "size = np.arange(500,X_train.shape[0], 500)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "blind_scores = []\n",
    "\n",
    "for i in size:\n",
    "    m.fit(X_train.iloc[:i,:].values, y_train.iloc[:i,:].values.ravel())\n",
    "    train_scores.append(metrics.r2_score(y_train.iloc[:i,:].values, m.predict(X_train.iloc[:i,:].values)))\n",
    "    test_scores.append(metrics.r2_score(y_test.values, m.predict(X_test)))\n",
    "    blind_scores.append(metrics.r2_score(y_blind.values, m.predict(X_blind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(size, train_scores, label = 'train')\n",
    "plt.plot(size, test_scores, label = 'test')\n",
    "plt.plot(size, blind_scores, label = 'blind')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to save images\n",
    "if os.path.exists(r'./Images'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(r'./Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs2(data, well_name, model_gb, model_svm, model_nn, formation):\n",
    "    \"\"\"\n",
    "    function to plot the log data and the predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The well data to be plotted\n",
    "    well_name : str\n",
    "        The name of the well being plotted\n",
    "    model:\n",
    "        The trained model used for the prediction\n",
    "    formation : dict\n",
    "        The formation tops ( names as keys and depth interval as the item in a list)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A plot of the well logs\n",
    "    \"\"\"   \n",
    "    #assigning the logs to variable names to make the code cleaner and easier to read\n",
    "    MD = data.DEPT\n",
    "    GR = data.GR\n",
    "    RHOB = data.RHOZ\n",
    "    NPHI = data.NPHI\n",
    "    DT= data.DTCO\n",
    "    PEFZ = data.PEFZ\n",
    "    BA = data.Brittleness_new\n",
    "\n",
    "    #creating the figure\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=6,figsize=(15,10), sharey=True, gridspec_kw={'width_ratios': [3,3,3,3,3,3]})\n",
    "        \n",
    "#     fig.suptitle(\"O  {}\".format(well_name), fontsize=25)\n",
    "    fig.subplots_adjust(top=0.85, wspace=0.2)\n",
    "\n",
    "#     ax[0].set_ylim(formation['Upper Marcellus'][0],formation['Lower Marcellus'][1])  #display only a depth range\n",
    "    ax[0].set_ylim(7600, formation['Lower Marcellus'][1])  #display only a depth range\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_ylabel('MD (M)',fontsize=20)\n",
    "    ax[0].yaxis.grid(True)\n",
    "    ax[0].get_xaxis().set_visible(False) #removing the x-axis label at the bottom of the fig\n",
    "\n",
    "    ##Track 1\n",
    "    ##Gamma_ray and PEF \n",
    "    ax_GR = ax[0].twiny()  #share the depth axis\n",
    "    ax_GR.set_xlim(0,270)\n",
    "    ax_GR.plot(GR,MD, color='black')\n",
    "    ax_GR.set_xlabel('GR (API)',color='black')\n",
    "    ax_GR.tick_params('x',colors='black')  ##change the color of the x-axis tick label\n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].yaxis.grid(True)\n",
    "    ax_GR.grid(True,alpha=0.5)\n",
    "\n",
    "    #variable colorfill\n",
    "    GR_range = abs(GR.min() - GR.max())\n",
    "    cmap = plt.get_cmap('nipy_spectral')   #color map\n",
    "    color_index = np.arange(GR.min(), GR.max(), GR_range / 20)\n",
    "\n",
    "    #loop through each value in the color_index\n",
    "    for index in sorted(color_index):\n",
    "        index_value = (index - GR.min())/GR_range\n",
    "        color = cmap(index_value) #obtain colour for color index value\n",
    "        ax_GR.fill_betweenx(MD, 0 , GR, where = GR >= index,  color = color)\n",
    "\n",
    "\n",
    "    ax_PEFZ = ax[0].twiny()\n",
    "    ax_PEFZ.plot(PEFZ,MD, color='red')\n",
    "    ax_PEFZ.set_xlabel('PEFZ',color='red')\n",
    "    ax_PEFZ.tick_params('x',colors='red')  ##change the color of the x-axis tick label\n",
    "    ax_PEFZ.spines['top'].set_position(('outward',40)) ##move the x-axis up\n",
    "    ax_PEFZ.spines[\"top\"].set_edgecolor(\"red\")\n",
    "\n",
    "    #Track 2\n",
    "    ##NPHI and RHOB\n",
    "    ax_NPHI = ax[1].twiny()\n",
    "    ax_NPHI.set_xlim(-0.1,0.4)\n",
    "    ax_NPHI.invert_xaxis()\n",
    "    ax_NPHI.plot(NPHI, MD, label='NPHI[%]', color='green') \n",
    "    ax_NPHI.spines['top'].set_position(('outward',0))\n",
    "    ax_NPHI.set_xlabel('NPHI[%]', color='green')    \n",
    "    ax_NPHI.tick_params(axis='x', colors='green')\n",
    "    ax_NPHI.spines[\"top\"].set_edgecolor(\"green\")\n",
    "\n",
    "    ax_RHOB = ax[1].twiny()\n",
    "    ax_RHOB.set_xlim(1.95,2.95)\n",
    "    ax_RHOB.invert_xaxis()\n",
    "    ax_RHOB.plot(RHOB, MD,label='RHOB[g/cc]', color='red') \n",
    "    ax_RHOB.spines['top'].set_position(('outward',40))\n",
    "    ax_RHOB.set_xlabel('RHOB[g/cc]',color='red')\n",
    "    ax_RHOB.tick_params(axis='x', colors='red')\n",
    "    ax_RHOB.spines[\"top\"].set_edgecolor('red')\n",
    "\n",
    "    ax[1].get_xaxis().set_visible(False)\n",
    "    ax[1].yaxis.grid(True)\n",
    "    ax_RHOB.grid(True,alpha=0.5)\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # #color fill\n",
    "    # x = np.array(ax_RHOB.get_xlim())\n",
    "    # z = np.array(ax_NPHI.get_xlim())\n",
    "\n",
    "    # nz=((NPHI-np.max(z))/(np.min(z)-np.max(z)))*(np.max(x)-np.min(x))+np.min(x)\n",
    "\n",
    "    # ax_RHOB.fill_betweenx(MD, RHOB, nz, where=RHOB>=nz, interpolate=True, color='green')\n",
    "    # ax_RHOB.fill_betweenx(MD, RHOB, nz, where=RHOB<=nz, interpolate=True, color='yellow')\n",
    "    \n",
    "\n",
    "    #Track 3\n",
    "    ##Sonic \n",
    "    ax_DT = ax[2].twiny()\n",
    "    ax_DT.grid(True)\n",
    "    ax_DT.set_xlim(100,50)\n",
    "    ax_DT.spines['top'].set_position(('outward',0))\n",
    "    ax_DT.plot(DT, MD, label='DT[us/ft]', color='blue')\n",
    "    ax_DT.set_xlabel('DT[us/ft]', color='blue')    \n",
    "    ax_DT.tick_params(axis='x', colors='blue')\n",
    "    ax_DT.spines[\"top\"].set_edgecolor(\"blue\")\n",
    "\n",
    "    ax[2].get_xaxis().set_visible(False)\n",
    "    ax[2].yaxis.grid(True)\n",
    "    ax_DT.grid(True,alpha=0.5)\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    #Track 4\n",
    "    #gb model\n",
    "    ax_BA1 = ax[3].twiny()\n",
    "    ax_BA1.grid(True)\n",
    "    ax_BA1.set_xlim(0,1)\n",
    "    ax_BA1.spines['top'].set_position(('outward',0))\n",
    "    ax_BA1.plot(BA, MD, label='BRITTLENESS ESTIMATE', color='black')\n",
    "    ax_BA1.set_xlabel('BRITTLENESS ESTIMATE', color='black')    \n",
    "    ax_BA1.tick_params(axis='x', colors='black')\n",
    "\n",
    "    ##Ploting the predicted data\n",
    "    ###work on this for generalization\n",
    "    ax_pred = ax[3].twiny()\n",
    "    df = data.loc[: , features].dropna()\n",
    "    pred = model_gb.predict(df.drop(['DEPT','RHOZ',target], axis=1))\n",
    "    df['Brittleness_predict'] = pred  \n",
    "    ax_BA1.plot(df.Brittleness_predict, df.DEPT, color='red', linestyle='--')\n",
    "\n",
    "    ax_pred.spines['top'].set_position(('outward',40))\n",
    "    ax_pred.set_xlabel('BRITTLENESS (GB)',color='red')\n",
    "    ax_pred.tick_params(axis='x', colors='red')\n",
    "    ax_pred.spines[\"top\"].set_edgecolor('red')\n",
    "\n",
    "\n",
    "    ax[3].get_xaxis().set_visible(False)\n",
    "    ax[3].yaxis.grid(True)\n",
    "    ax[3].axis('off')\n",
    "    ax_BA1.grid(True,alpha=0.5)\n",
    "\n",
    "    #Track 4\n",
    "    ##Brittleness\n",
    "    ## nn model\n",
    "    ax_BA2 = ax[4].twiny()\n",
    "    ax_BA2.grid(True)\n",
    "    ax_BA2.set_xlim(0,1)\n",
    "    ax_BA2.spines['top'].set_position(('outward',0))\n",
    "    ax_BA2.plot(BA, MD, label='BRITTLENESS ESTIMATE', color='black')\n",
    "    ax_BA2.set_xlabel('BRITTLENESS ESTIMATE', color='black')    \n",
    "    ax_BA2.tick_params(axis='x', colors='black')\n",
    "\n",
    "    ##Ploting the predicted data\n",
    "    ###work on this for generalization\n",
    "    ax_pred = ax[4].twiny()\n",
    "    df = data.loc[: , features].dropna()\n",
    "    pred = model_nn.predict(df.drop(['DEPT','RHOZ', target], axis=1))\n",
    "    df['Brittleness_predict'] = pred  \n",
    "    ax_BA2.plot(df.Brittleness_predict, df.DEPT, color='blue', linestyle='--')\n",
    "\n",
    "    ax_pred.spines['top'].set_position(('outward',40))\n",
    "    ax_pred.set_xlabel('BRITTLENESS (NN)',color='blue')\n",
    "    ax_pred.tick_params(axis='x', colors='blue')\n",
    "    ax_pred.spines[\"top\"].set_edgecolor('blue')\n",
    "\n",
    "\n",
    "    ax[4].get_xaxis().set_visible(False)\n",
    "    ax[4].yaxis.grid(True)\n",
    "    ax[4].axis('off')\n",
    "    ax_BA2.grid(True,alpha=0.5)\n",
    "\n",
    "    \n",
    "    #Track 4\n",
    "    ##Brittleness\n",
    "    ##svm model\n",
    "    ax_BA3 = ax[5].twiny()\n",
    "    ax_BA3.grid(True)\n",
    "    ax_BA3.set_xlim(0,1)\n",
    "    ax_BA3.spines['top'].set_position(('outward',0))\n",
    "    ax_BA3.plot(BA, MD, label='BRITTLENESS ESTIMATE', color='black')\n",
    "    ax_BA3.set_xlabel('BRITTLENESS ESTIMATE', color='black')    \n",
    "    ax_BA3.tick_params(axis='x', colors='black')\n",
    "\n",
    "    ##Ploting the predicted data\n",
    "    ###work on this for generalization\n",
    "    ax_pred = ax[5].twiny()\n",
    "    df = data.loc[: , features].dropna()\n",
    "    pred = model_svm.predict(df.drop(['DEPT','RHOZ',target], axis=1))\n",
    "    df['Brittleness_predict'] = pred  \n",
    "    ax_BA3.plot(df.Brittleness_predict, df.DEPT, color='purple', linestyle='--')\n",
    "\n",
    "    ax_pred.spines['top'].set_position(('outward',40))\n",
    "    ax_pred.set_xlabel('BRITTLENESS (SVM)',color='purple')\n",
    "    ax_pred.tick_params(axis='x', colors='purple')\n",
    "    ax_pred.spines[\"top\"].set_edgecolor('purple')\n",
    "\n",
    "\n",
    "    ax[5].get_xaxis().set_visible(False)\n",
    "    ax[5].yaxis.grid(True)\n",
    "    ax[5].axis('off')\n",
    "    ax_BA3.grid(True,alpha=0.5)\n",
    "\n",
    "    \n",
    "#     #formation top\n",
    "#     ax_top = ax[-1]\n",
    "#     ax[-1].axis('off')\n",
    "\n",
    "#     formation_midpoints = []\n",
    "#     for key, value in formation.items():\n",
    "#         #Calculate mid point of the formation\n",
    "#         formation_midpoints.append(value[0] + (value[1]-value[0])/2)\n",
    "\n",
    "#     zone_colours = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "#     for ax in [ax_GR, ax_NPHI, ax_BA1, ax_BA2, ax_BA3, ax_top]:\n",
    "#         # loop through the formations dictionary and zone colours\n",
    "#         for depth, colour in zip(formation.values(), zone_colours):\n",
    "#             # use the depths and colours to shade across the subplots\n",
    "#             ax.axhspan(depth[0], depth[1], color=colour, alpha=0.1)\n",
    "\n",
    "#     for label, formation_mid in zip(formation.keys(), \n",
    "#                                         formation_midpoints):\n",
    "#         ax_top.text(0.5, formation_mid, label, rotation=90,\n",
    "#                     verticalalignment='center', horizontalalignment='center', fontweight='bold',\n",
    "#                     fontsize='large')\n",
    "#     fig.savefig(r'./Images/{}.png'.format(well_name), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation = {'Tully': [7195,7310],\n",
    "#              'Mahantango': [7310,7455],\n",
    "#             'Marcellus': [7455,7560]}\n",
    "\n",
    "formation = {'Upper Marcellus': [7453,7476],\n",
    "             'Middle Marcellus': [7476,7517],\n",
    "            'Lower Marcellus': [7517,7555]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logs2(data_mip3h, \"MIP3H2\", model_gb, model_svm, model_nn, formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation2 = {'Tully': [7604,7670],\n",
    "#              'Mahantango': [7670,7882],\n",
    "#             'Marcellus': [7882,8052]}\n",
    "formation_pos = {'Upper Marcellus': [7883,7961],\n",
    "             'Middle Marcellus': [7961,8015],\n",
    "            'Lower Marcellus': [8015,8052]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_logs2(data_poseidon, \"Poseidon\", model_gb, model_svm, model_nn, formation_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_bog = {'Upper Marcellus': [7877,7905],\n",
    "             'Middle Marcellus': [7905, 7951],\n",
    "            'Lower Marcellus': [7951,7974]}\n",
    "plot_logs2(data_boggess, \"Boggess\", model_gb, model_svm, model_nn, formation_bog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_whip = {'Upper Marcellus': [7736, 7785],\n",
    "             'Middle Marcellus': [7785, 7811],\n",
    "            'Lower Marcellus': [7811, 7835]}\n",
    "plot_logs2(data_whipkey, \"Whipkey\", model_gb, model_svm, model_nn, formation_whip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
