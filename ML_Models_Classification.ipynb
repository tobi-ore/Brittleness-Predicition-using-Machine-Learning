{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning to Predict Brittleness from other Geophysical Logs\n",
    "\n",
    "## Data: 4 wells from the Appalachian Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbR, GradientBoostingClassifier as gbC, IsolationForest\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "pd.set_option('display.max_columns', None)   #to display all the column information\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_directory = r\"C:\\Users\\tmo0005\\Desktop\\Thesis work\\Thesis work\\Well_Data_CSV_Merged\"  #for lab computer\n",
    "\n",
    "file_directory = r\"../Thesis work/Thesis work/Well_Data_CSV_Merged\"    #for macbook google drive\n",
    "\n",
    "file_name1 = \"Poseidon.csv\"\n",
    "file_name2 = \"Boggess.csv\"\n",
    "file_name3 = \"Mip3h.csv\"\n",
    "file_name4 = \"Whipkey.csv\"\n",
    "\n",
    "file_name = [file_name1, file_name2, file_name3, file_name4]\n",
    "data = []\n",
    "\n",
    "for i in file_name:\n",
    "    file_path = os.path.join(file_directory,i)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_poseidon = data[0]\n",
    "data_boggess = data[1]\n",
    "data_mip3h = data[2]\n",
    "data_whipkey = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Poseidon data has {} rows\".format(data_poseidon.shape[0]))\n",
    "print(\"The Boggess data has {} rows\".format(data_boggess.shape[0]))\n",
    "print(\"The Mip3h data has {} rows\".format(data_mip3h.shape[0]))\n",
    "print(\"The Whipkey data has {} rows\".format(data_whipkey.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_whipkey, \n",
    "                  data_boggess,\n",
    "                  data_mip3h], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['DEPT', 'GR', 'NPHI','RHOZ', 'HCAL', 'DTCO','PEFZ', 'Brittleness_new']   #list of the features names to select\n",
    "target = 'Brittleness_new'   #name of the output feature\n",
    "data = data.loc[: ,features]\n",
    "data[data < 0] = np.nan  #remove negative values\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cat = 'Brittleness_Ranking'\n",
    "bins = [0,0.25,1.0]    #group interval for the brittleness classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_2_cat(data,bins,target,target_cat):\n",
    "    data2 = data.copy()\n",
    "    category = pd.cut(data2[target],bins=bins,labels=np.arange(1,len(bins)))\n",
    "    data2.insert(0,target_cat,category)\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = reg_2_cat(data, bins, target, target_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.Brittleness_Ranking.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = data2.drop([target,target_cat], axis=1)\n",
    "y2 = data2.loc[:,[target_cat]]\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size=0.3, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train2)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train2, y_train2 = X_train2[mask], y_train2[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train2.shape, y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_poseidon2 = reg_2_cat(data_poseidon, bins, target, target_cat)\n",
    "data_boggess2 = reg_2_cat(data_boggess, bins, target, target_cat)\n",
    "data_mip3h2 = reg_2_cat(data_mip3h, bins, target, target_cat)\n",
    "data_whipkey2 = reg_2_cat(data_whipkey, bins, target, target_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_temp = features\n",
    "features_temp.append(target_cat)\n",
    "df = data_poseidon2.loc[: , features_temp]\n",
    "df.dropna(inplace=True)\n",
    "X_blind2 = df.drop([target, target_cat], axis=1)\n",
    "y_blind2 = df.loc[:,[target_cat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_whipkey2.Brittleness_new)\n",
    "plt.axvline(0.2, color = 'r')\n",
    "plt.axvline(0.3, color = 'r')\n",
    "plt.xlabel(\"Brittleness Estimate\")\n",
    "plt.ylabel(\"frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(X_train, X_test, X_blind, y_train, y_test, y_blind, algorithm, hyper_parameters, scaler, classification,printFeatureImportance=True, cv_folds=3):\n",
    "    \"\"\"\n",
    "    function to tune the gradient boosting model and return the optimum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : DataFrame\n",
    "        The input features for the training set\n",
    "    X_test : DataFrame\n",
    "        The input features for the testing set\n",
    "    X_blind : DataFrame\n",
    "        The input features for the blind set\n",
    "    y_train : DataFrame \n",
    "        The output feature for the training set\n",
    "    y_test : DataFrame \n",
    "        The output feature for the testing set\n",
    "    y_blind : DataFrame \n",
    "        The output feature for the blind set\n",
    "    algorithm : {'neural','svm','gradientboosting'}\n",
    "        The Machine Learning model \n",
    "    hyper_parameters : dict\n",
    "        A dictionary of the hyperparameters of the models that will be tuned\n",
    "    scaler : {'standard','minmax'}\n",
    "        Scaling technique to employ.\n",
    "    classification : bool \n",
    "        Flag to specify the modeling technique. True for classification and False for regression\n",
    "    printFeatureImportance : bool\n",
    "        Flag to specify if to display the feature importance histogram.\n",
    "    cv_folds : int\n",
    "        Number of cross-validation folds. default is 3.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : an object of the trained gradient boosting which can be deployed or saved\n",
    "    \"\"\"   \n",
    "    #step to assign the selected standardaziation \n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler == 'minmax':\n",
    "        scaler =MinMaxScaler()\n",
    "    else:\n",
    "        print(\"invalid scaler: use 'standard' or 'minmax'\")\n",
    "        \n",
    "    \n",
    "    #step to assign the selected machine learning algorithm\n",
    "    if algorithm == 'svm':\n",
    "        if classification is True:\n",
    "            algo = SVC(random_state=83)\n",
    "        else:\n",
    "            algo = SVR()\n",
    "        \n",
    "    elif algorithm == 'neural':\n",
    "        if classification is True:\n",
    "            algo = MLPClassifier(random_state=677)\n",
    "        else:\n",
    "            algo = MLPRegressor(random_state=134)\n",
    "    \n",
    "    elif algorithm == 'gradientboosting':\n",
    "        if classification is True:\n",
    "            algo = gbC(random_state=10)\n",
    "        else:\n",
    "            algo = gbR(random_state=824)\n",
    "    else:\n",
    "        print(\"invalid scaler: use 'svm' or 'neural' or 'gradientboosting'\")\n",
    "    \n",
    "     \n",
    "        \n",
    "    if classification is True:\n",
    "        pipe = Pipeline(steps=[('scaler', scaler), ('model', algo)])\n",
    "        model = GridSearchCV(estimator = pipe,\n",
    "                             param_grid = hyper_parameters,\n",
    "                             scoring='accuracy',n_jobs=-1, cv=cv_folds, verbose = 1)\n",
    "        \n",
    "        #Fit the model on the data\n",
    "        model.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "        #Predict training set:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "        #Predict testing set:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        #Predict blind set\n",
    "        y_blind_pred = model.predict(X_blind)\n",
    "\n",
    "        #Print model report:\n",
    "        print(\"Model Report\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"The training accuracy : {0:.4g}\".format(metrics.accuracy_score(y_train.values, y_train_pred)))\n",
    "        print(\"The testing accuracy is : {0:.4g}\".format(metrics.accuracy_score(y_test.values,y_test_pred)))\n",
    "        print(\"The blind well accuracy is : {0:.4g}\".format(metrics.accuracy_score(y_blind.values,y_blind_pred)))\n",
    "        print(\"CV best score : {0:.4g}\".format(model.best_score_))\n",
    "        print(\"CV best parameter combinations : {}\".format(model.best_params_))\n",
    "        \n",
    "        if algorithm == 'gradientboosting':\n",
    "            #Print Feature Importance:\n",
    "            if printFeatureImportance:\n",
    "                feat_imp = pd.Series(model.best_estimator_.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "                feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "                plt.xlabel('Feature Importance Score')\n",
    "           \n",
    "    else:\n",
    "        pipe = Pipeline(steps=[('scaler', scaler), ('model', algo)])\n",
    "        model = GridSearchCV(estimator = pipe,\n",
    "                             param_grid = hyper_parameters,\n",
    "                             scoring='r2',n_jobs=-1,\n",
    "                             cv=cv_folds, verbose = 1)\n",
    "        \n",
    "        #Fit the model on the data\n",
    "        model.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "        #Predict training set:\n",
    "        y_train_pred = model.predict(X_train)\n",
    "\n",
    "        #Predict testing set:\n",
    "        y_test_pred = model.predict(X_test)\n",
    "                        \n",
    "        #Predict blind set\n",
    "        y_blind_pred = model.predict(X_blind)\n",
    "\n",
    "        #Print model report:\n",
    "        print(\"Model Report\")\n",
    "        print(\"-------------------------------\")\n",
    "        print(\"The training R2 score : {0:.4g}\".format(metrics.r2_score(y_train.values, y_train_pred)))\n",
    "        print(\"The testing R2 score is : {0:.4g}\".format(metrics.r2_score(y_test.values,y_test_pred)))\n",
    "        print(\"The blind well R2 score is : {0:.4g}\".format(metrics.r2_score(y_blind.values,y_blind_pred)))\n",
    "        print(\"CV best score : {0:.4g}\".format(model.best_score_))\n",
    "        print(\"CV best parameter combinations : {}\".format(model.best_params_))\n",
    "        \n",
    "        if algorithm == 'gradientboosting':\n",
    "            #Print Feature Importance:\n",
    "            if printFeatureImportance:\n",
    "                feat_imp = pd.Series(model.best_estimator_.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "                feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "                plt.xlabel('Feature Importance Score')\n",
    "\n",
    "    return model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of SVR() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters =  {\"model__min_samples_split\" : [3],\n",
    "                     \"model__min_samples_leaf\": [1,2,3],\n",
    "                     \"model__max_depth\"        : [3, 4, 5,6,7,8,]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = modelfit(X_train2, X_test2, X_blind2, y_train2, y_test2, y_blind2, algorithm='gradientboosting', \n",
    "             hyper_parameters=hyper_parameters, scaler='standard', \n",
    "             classification=True,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp = pd.Series(model_gb.named_steps.model.feature_importances_, X_train.columns).sort_values(ascending=False)\n",
    "# feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "# plt.xlabel('Feature Importance Score')\n",
    "# # plt.savefig(r'./Images/{}.png'.format('gb_feature_importance'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of SVR() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters = {\n",
    "                    'model__C': np.logspace(0,2,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_svm = modelfit(X_train2, X_test2, X_blind2, y_train2, y_test2, y_blind2, algorithm='svm', \n",
    "         hyper_parameters=hyper_parameters, scaler='standard', \n",
    "         classification=True,printFeatureImportance=False, cv_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the documentation of MLPClassifier() to understand the parameters\n",
    "#put new parameters in the grid by using \"model__\" before the parameter name as below\n",
    "hyper_parameters =  {'model__hidden_layer_sizes': [(10,10,),(19,19,),(20,),(20,20,)],\n",
    "                     'model__tol': [0.0001,0.00001,0.001],\n",
    "                    'model__solver': ['lbfgs'],\n",
    "                    'model__max_iter': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = modelfit(X_train2, X_test2, X_blind2, y_train2, y_test2, y_blind2, algorithm='neural', \n",
    "         hyper_parameters=hyper_parameters, scaler='minmax', \n",
    "         classification=True,printFeatureImportance=True, cv_folds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder to save images\n",
    "if os.path.exists(r'./Images'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(r'./Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(data, well_name, model_gb, model_svm, model_nn, formation, classification):\n",
    "    \"\"\"\n",
    "    function to plot the log data and the predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The well data to be plotted\n",
    "    well_name : str\n",
    "        The name of the well being plotted\n",
    "    model:\n",
    "        The trained model used for the prediction\n",
    "    formation : dict\n",
    "        The formation tops ( names as keys and depth interval as the item in a list)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A plot of the well logs\n",
    "    \"\"\"         \n",
    "    data = data.loc[: , ['DEPT', 'GR', 'NPHI','RHOZ', 'HCAL', 'DTCO','PEFZ', 'Brittleness_new', 'Brittleness_Ranking']].dropna()\n",
    "    pred = model_gb.predict(data.drop(['Brittleness_new', 'Brittleness_Ranking'], axis=1))\n",
    "    data['Brittleness_predict_GB'] = pred  \n",
    "    \n",
    "    pred = model_svm.predict(data.drop(['Brittleness_new', 'Brittleness_Ranking','Brittleness_predict_GB'], axis=1))\n",
    "    data['Brittleness_predict_SVM'] = pred \n",
    "    \n",
    "    pred = model_nn.predict(data.drop(['Brittleness_new', 'Brittleness_Ranking','Brittleness_predict_GB', 'Brittleness_predict_SVM'], axis=1))\n",
    "    data['Brittleness_predict_NN'] = pred\n",
    "    \n",
    "    \n",
    "    #assigning the logs to variable names to make the code cleaner and easier to read\n",
    "    MD = data.DEPT\n",
    "    GR = data.GR\n",
    "    RHOB = data.RHOZ\n",
    "    NPHI = data.NPHI\n",
    "    DT= data.DTCO\n",
    "    PEFZ = data.PEFZ\n",
    "    BA = data.Brittleness_new\n",
    "\n",
    "    #creating the figure\n",
    "    if classification is True:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=9,figsize=(15,10), sharey=True, gridspec_kw={'width_ratios': [3,3,3,3,2,2,2,2,1]})\n",
    "    else:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=5,figsize=(12,10), sharey=True, gridspec_kw={'width_ratios': [3,3,3,3,1]})\n",
    "        \n",
    "#     fig.suptitle(\"{} Well Log Display\".format(well_name), fontsize=25)\n",
    "    fig.subplots_adjust(top=0.85, wspace=0.05)\n",
    "\n",
    "    ax[0].set_ylim(formation['Upper Marcellus'][0],formation['Lower Marcellus'][1])  #display only a depth range\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_ylabel('MD (M)',fontsize=20)\n",
    "    ax[0].yaxis.grid(True)\n",
    "    ax[0].get_xaxis().set_visible(False) #removing the x-axis label at the bottom of the fig\n",
    "\n",
    "    ##Track 1\n",
    "    ##Gamma_ray and PEF \n",
    "    ax_GR = ax[0].twiny()  #share the depth axis\n",
    "    ax_GR.set_xlim(0,270)\n",
    "    ax_GR.plot(GR,MD, color='black')\n",
    "    ax_GR.set_xlabel('GR (API)',color='black')\n",
    "    ax_GR.tick_params('x',colors='black')  ##change the color of the x-axis tick label\n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].yaxis.grid(True)\n",
    "    ax_GR.grid(True,alpha=0.5)\n",
    "\n",
    "    #variable colorfill\n",
    "    GR_range = abs(GR.min() - GR.max())\n",
    "    cmap = plt.get_cmap('nipy_spectral')   #color map\n",
    "    color_index = np.arange(GR.min(), GR.max(), GR_range / 20)\n",
    "\n",
    "    #loop through each value in the color_index\n",
    "    for index in sorted(color_index):\n",
    "        index_value = (index - GR.min())/GR_range\n",
    "        color = cmap(index_value) #obtain colour for color index value\n",
    "        ax_GR.fill_betweenx(MD, 0 , GR, where = GR >= index,  color = color)\n",
    "\n",
    "\n",
    "    ax_PEFZ = ax[0].twiny()\n",
    "    ax_PEFZ.plot(PEFZ,MD, color='red')\n",
    "    ax_PEFZ.set_xlabel('PEFZ',color='red')\n",
    "    ax_PEFZ.tick_params('x',colors='red')  ##change the color of the x-axis tick label\n",
    "    ax_PEFZ.spines['top'].set_position(('outward',40)) ##move the x-axis up\n",
    "    ax_PEFZ.spines[\"top\"].set_edgecolor(\"red\")\n",
    "\n",
    "    #Track 2\n",
    "    ##NPHI and RHOB\n",
    "    ax_NPHI = ax[1].twiny()\n",
    "    ax_NPHI.set_xlim(-0.1,0.4)\n",
    "    ax_NPHI.invert_xaxis()\n",
    "    ax_NPHI.plot(NPHI, MD, label='NPHI[%]', color='green') \n",
    "    ax_NPHI.spines['top'].set_position(('outward',0))\n",
    "    ax_NPHI.set_xlabel('NPHI[%]', color='green')    \n",
    "    ax_NPHI.tick_params(axis='x', colors='green')\n",
    "    ax_NPHI.spines[\"top\"].set_edgecolor(\"green\")\n",
    "\n",
    "    ax_RHOB = ax[1].twiny()\n",
    "    ax_RHOB.set_xlim(1.95,2.95)\n",
    "    ax_RHOB.invert_xaxis()\n",
    "    ax_RHOB.plot(RHOB, MD,label='RHOB[g/cc]', color='red') \n",
    "    ax_RHOB.spines['top'].set_position(('outward',40))\n",
    "    ax_RHOB.set_xlabel('RHOB[g/cc]',color='red')\n",
    "    ax_RHOB.tick_params(axis='x', colors='red')\n",
    "    ax_RHOB.spines[\"top\"].set_edgecolor('red')\n",
    "\n",
    "    ax[1].get_xaxis().set_visible(False)\n",
    "    ax[1].yaxis.grid(True)\n",
    "    ax_RHOB.grid(True,alpha=0.5)\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # #color fill\n",
    "    # x = np.array(ax_RHOB.get_xlim())\n",
    "    # z = np.array(ax_NPHI.get_xlim())\n",
    "\n",
    "    # nz=((NPHI-np.max(z))/(np.min(z)-np.max(z)))*(np.max(x)-np.min(x))+np.min(x)\n",
    "\n",
    "    # ax_RHOB.fill_betweenx(MD, RHOB, nz, where=RHOB>=nz, interpolate=True, color='green')\n",
    "    # ax_RHOB.fill_betweenx(MD, RHOB, nz, where=RHOB<=nz, interpolate=True, color='yellow')\n",
    "\n",
    "    #Track 3\n",
    "    ##Sonic \n",
    "    ax_DT = ax[2].twiny()\n",
    "    ax_DT.grid(True)\n",
    "    ax_DT.set_xlim(100,50)\n",
    "    ax_DT.spines['top'].set_position(('outward',0))\n",
    "    ax_DT.plot(DT, MD, label='DT[us/ft]', color='blue')\n",
    "    ax_DT.set_xlabel('DT[us/ft]', color='blue')    \n",
    "    ax_DT.tick_params(axis='x', colors='blue')\n",
    "    ax_DT.spines[\"top\"].set_edgecolor(\"blue\")\n",
    "\n",
    "    ax[2].get_xaxis().set_visible(False)\n",
    "    ax[2].yaxis.grid(True)\n",
    "    ax_DT.grid(True,alpha=0.5)\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    #Track 4\n",
    "    ##Brittleness\n",
    "    ax_BA = ax[3].twiny()\n",
    "    ax_BA.grid(True)\n",
    "    ax_BA.set_xlim(0,1)\n",
    "    ax_BA.spines['top'].set_position(('outward',0))\n",
    "    ax_BA.plot(BA, MD, label='BRITTLENESS ESTIMATE', color='black')\n",
    "    ax_BA.set_xlabel('BRITTLENESS ESTIMATE', color='black')    \n",
    "    ax_BA.tick_params(axis='x', colors='black')\n",
    "    ax[3].get_xaxis().set_visible(False)\n",
    "    \n",
    "    \n",
    "    #variable colorfill\n",
    "    BA_range = abs(BA.min() - BA.max())\n",
    "    cmap = plt.get_cmap('nipy_spectral')   #color map\n",
    "    color_index = np.arange(BA.min(), BA.max(), BA_range / 20)\n",
    "    \n",
    "    #loop through each value in the color_index\n",
    "    for index in sorted(color_index):\n",
    "        index_value = (index - BA.min())/BA_range\n",
    "        color = cmap(index_value) #obtain colour for color index value\n",
    "        ax_BA.fill_betweenx(MD, 0 , BA, where = BA >= index,  color = color)\n",
    "\n",
    "\n",
    "    ##Ploting the predicted data\n",
    "    ###work on this for generalization\n",
    "    if classification is True:\n",
    "        cmap_facies = colors.ListedColormap(['orange','brown', 'green'], 'indexed')\n",
    "\n",
    "        ax_BA_class = ax[4].twiny()\n",
    "        cluster=np.repeat(np.expand_dims(data['Brittleness_Ranking'].values,1), 5, 1)\n",
    "        im=ax_BA_class.imshow(cluster, interpolation='none', cmap=cmap_facies, aspect='auto',\n",
    "                        vmin=data['Brittleness_Ranking'].min(),vmax=data['Brittleness_Ranking'].max(), \n",
    "                     extent=[0,5, data.DEPT.max(), data.DEPT.min()])\n",
    "        ax_BA_class.set_xlabel(\"ACTUAL\")\n",
    "\n",
    "        ax_BA_class_pred1 = ax[5].twiny()\n",
    "        cluster=np.repeat(np.expand_dims(data['Brittleness_predict_GB'].values,1), 5, 1)\n",
    "        im=ax_BA_class_pred1.imshow(cluster, interpolation='none', cmap=cmap_facies, aspect='auto',\n",
    "                        vmin=data['Brittleness_predict_GB'].min(),vmax=data['Brittleness_predict_GB'].max(), \n",
    "                     extent=[0,5, data.DEPT.max(), data.DEPT.min()])\n",
    "        ax_BA_class_pred1.set_xlabel(\"GB\")\n",
    "        \n",
    "        ax_BA_class_pred2 = ax[6].twiny()\n",
    "        cluster=np.repeat(np.expand_dims(data['Brittleness_predict_SVM'].values,1), 5, 1)\n",
    "        im=ax_BA_class_pred2.imshow(cluster, interpolation='none', cmap=cmap_facies, aspect='auto',\n",
    "                        vmin=data['Brittleness_predict_SVM'].min(),vmax=data['Brittleness_predict_SVM'].max(), \n",
    "                     extent=[0,5, data.DEPT.max(), data.DEPT.min()])\n",
    "        ax_BA_class_pred2.set_xlabel(\"SVM\")\n",
    "        \n",
    "        ax_BA_class_pred3 = ax[7].twiny()\n",
    "        cluster=np.repeat(np.expand_dims(data['Brittleness_predict_NN'].values,1), 5, 1)\n",
    "        im=ax_BA_class_pred3.imshow(cluster, interpolation='none', cmap=cmap_facies, aspect='auto',\n",
    "                        vmin=data['Brittleness_predict_NN'].min(),vmax=data['Brittleness_predict_NN'].max(), \n",
    "                     extent=[0,5, data.DEPT.max(), data.DEPT.min()])\n",
    "        ax_BA_class_pred3.set_xlabel(\"NN\")\n",
    "        \n",
    "        \n",
    "        ax[4].get_xaxis().set_visible(False)\n",
    "        ax[5].get_xaxis().set_visible(False)\n",
    "        ax[6].get_xaxis().set_visible(False)\n",
    "        ax[7].get_xaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        ax_pred = ax[3].twiny()\n",
    "        df = data.loc[: , features].dropna()\n",
    "        pred = model.predict(df.drop([target], axis=1))\n",
    "        df['Brittleness_predict'] = pred  \n",
    "        ax_BA.plot(df.Brittleness_predict, df.DEPT, color='red', linestyle='--')\n",
    "\n",
    "        ax_pred.spines['top'].set_position(('outward',40))\n",
    "        ax_pred.set_xlabel('BRITTLENESS PREDICTED',color='red')\n",
    "        ax_pred.tick_params(axis='x', colors='red')\n",
    "        ax_pred.spines[\"top\"].set_edgecolor('red')\n",
    "\n",
    "        \n",
    "        ax[3].get_xaxis().set_visible(False)\n",
    "        ax[3].yaxis.grid(True)\n",
    "        ax[3].axis('off')\n",
    "        ax_BA.grid(True,alpha=0.5)\n",
    "\n",
    "\n",
    "    #formation top\n",
    "    ax_top = ax[-1]\n",
    "    ax[-1].axis('off')\n",
    "\n",
    "    formation_midpoints = []\n",
    "    for key, value in formation.items():\n",
    "        #Calculate mid point of the formation\n",
    "        formation_midpoints.append(value[0] + (value[1]-value[0])/2)\n",
    "\n",
    "    zone_colours = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "    for ax in [ax_GR, ax_NPHI, ax_DT, ax_BA, ax_top]:\n",
    "        # loop through the formations dictionary and zone colours\n",
    "        for depth, colour in zip(formation.values(), zone_colours):\n",
    "            # use the depths and colours to shade across the subplots\n",
    "            ax.axhspan(depth[0], depth[1], color=colour, alpha=0.1)\n",
    "\n",
    "    for label, formation_mid in zip(formation.keys(), \n",
    "                                        formation_midpoints):\n",
    "        ax_top.text(0.5, formation_mid, label, rotation=90,\n",
    "                    verticalalignment='center', horizontalalignment='center', fontweight='bold',\n",
    "                    fontsize='large')\n",
    "        \n",
    "#     fig.savefig(r'./Images/{}_strategy3.png'.format(well_name), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation = {'Upper Marcellus': [7453,7476],\n",
    "#              'Middle Marcellus': [7476,7517],\n",
    "#             'Lower Marcellus': [7517,7555]}\n",
    "\n",
    "# plot_logs(data_mip3h2, \"MIP3H\", model_gb, model_svm, model_nn, formation, classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation_pos = {'Upper Marcellus': [7883,7961],\n",
    "#              'Middle Marcellus': [7961,8015],\n",
    "#             'Lower Marcellus': [8015,8052]}\n",
    "\n",
    "# plot_logs(data_poseidon2,\"Poseidon\", model_gb, model_svm, model_nn, formation_pos, classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_bog = {'Upper Marcellus': [7877,7905],\n",
    "             'Middle Marcellus': [7905, 7951],\n",
    "            'Lower Marcellus': [7951,7974]}\n",
    "plot_logs(data_boggess2, \"Boggess\", model_gb, model_svm, model_nn, formation_bog, classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation_whip = {'Upper Marcellus': [7736, 7785],\n",
    "#              'Middle Marcellus': [7785, 7811],\n",
    "#             'Lower Marcellus': [7811, 7835]}\n",
    "# plot_logs(data_whipkey2, \"Whipkey\", model_gb, model_svm, model_nn, formation_whip, classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
